{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d795cffb-c2ca-429f-aff3-0e0d53ff43c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ce873b-0920-4186-a3b0-bfda8f4b77d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Prepare dataset\n",
    "#2. Load pretrained Tokenizer, call it with dataset -> encoding\n",
    "#3. Build Pytorch Dataset with encodings\n",
    "#4. Load pretrained Model\n",
    "#5. a) Load Trainer and train it\n",
    "#   b) or use native Pytorch training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0d98fe8-8c15-4110-9bdf-5ddf8c5e9196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31434ba7-7419-48f4-a024-cc64a3bfdf17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd169cd2ef545e3a4cdd7e46994cb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61bce1e9ff7442c0898b2d8da713d42e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8289cfc634d843f3b6355a4a6100b142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab9ae5bd268347ea90eef43179fe71b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'POSITIVE', 'score': 0.9996298551559448}\n",
      "{'label': 'NEGATIVE', 'score': 0.9600156545639038}\n"
     ]
    }
   ],
   "source": [
    "model_name =  \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# classifier = pipeline(\"sentiment-analysis\", model = model_name)\n",
    "# results = classifier([\"We are very happy to show tou the ☺️ face transformers library\",\"We hope you dont hate it\"])    \n",
    "# for result in results:\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76027c55-3645-4ead-b12d-b331ce5eb0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'POSITIVE', 'score': 0.9996298551559448}\n",
      "{'label': 'NEGATIVE', 'score': 0.9600156545639038}\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model = model_name)\n",
    "results = classifier([\"We are very happy to show tou the ☺️ face transformers library\",\n",
    "                      \"We hope you dont hate it\"])    \n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d29e2d8b-3b0e-40db-a189-ded3ade789d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens :['we', 'are', 'very', 'happy', 'to', 'show', 'you', 'the', '[UNK]', 'transformers', 'library']\n",
      "Tokens :[2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075]\n",
      "Tokens :{'input_ids': [101, 2057, 2024, 2200, 3407, 2000, 2265, 2017, 1996, 100, 19081, 3075, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"We are very happy to show you the ☺️ Transformers library\")\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = tokenizer(\"We are very happy to show you the ☺️ Transformers library\")\n",
    "\n",
    "print(f'Tokens :{tokens}')\n",
    "print(f'Tokens :{token_ids}')\n",
    "print(f'Tokens :{input_ids}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe1eb131-b233-430f-9457-e128f6cd632e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2057,  2024,  2200,  3407,  2000,  2265,  2000,  2226,  1996,\n",
      "           100,  2227, 19081,  3075,   102],\n",
      "        [  101,  2057,  3246,  2017,  2123,  2102,  5223,  2009,   102,     0,\n",
      "             0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "X_train = [\"We are very happy to show tou the ☺️ face transformers library\",\n",
    "                      \"We hope you dont hate it\"]\n",
    "batch = tokenizer(X_train, padding=True, truncation=True, max_length=512, return_tensors = \"pt\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4667a92-d044-41c1-9cba-fe456bbd2cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=tensor(0.0206), logits=tensor([[-3.8563,  4.0451],\n",
      "        [ 1.6841, -1.4944]]), hidden_states=None, attentions=None)\n",
      "tensor([[3.7010e-04, 9.9963e-01],\n",
      "        [9.6002e-01, 3.9984e-02]])\n",
      "tensor([1, 0])\n",
      "['POSITIVE', 'NEGATIVE']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**batch, labels = torch.tensor([1,0])) #unpack batch\n",
    "    print(outputs)\n",
    "    \n",
    "    predictions = F.softmax(outputs.logits, dim = 1)\n",
    "    print(predictions)\n",
    "    \n",
    "    labels = torch.argmax(predictions, dim = 1)\n",
    "    print(labels)\n",
    "    \n",
    "    labels =[model.config.id2label[label_id] for label_id in labels.tolist()]\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4921cf-8ee1-4783-bedd-96464447fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine-tune our model\n",
    "# save_directory = \"saved\"\n",
    "# tokenizer.save_pretrained(save_directory)\n",
    "# model.save_pretrained(save_directory)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15f76550-1da6-4e9e-a8b4-09dc182eb177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    3,   304,  8524,  5569,  2011, 26902,     4,     0,     0],\n",
      "        [    3,   295,   127,  2523,   149,  2723,   181,  1522,     4],\n",
      "        [    3, 19990,    18,  7117,  4741, 26982,     4,     0,     0],\n",
      "        [    3,   149,   181,  6975,   246,  6303,     4,     0,     0],\n",
      "        [    3,   233,  4496, 14332,  8453, 26914,     4,     0,     0],\n",
      "        [    3,   371,  9755,    39, 19044, 26902,  3512, 26914,     4]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([1, 1, 0, 0, 2, 2])\n",
      "['negative', 'negative', 'positive', 'positive', 'neutral', 'neutral']\n"
     ]
    }
   ],
   "source": [
    "model_name = \"oliverguhr/german-sentiment-bert\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "X_train_german = [\"Mit keinem guten Ergebniss\",\"Das ist gar nicht mal so gut\",\n",
    "    \"Total awesome!\",\"nicht so schlecht wie erwartet\",\n",
    "    \"Der Test verlief positiv.\",\"Sie fährt ein grünes Auto.\"]\n",
    "\n",
    "batch = tokenizer(X_train_german, padding=True, truncation=True, max_length=512, return_tensors = \"pt\")\n",
    "#batch = torch.tensor(batch[\"input_ids\"])\n",
    "print(batch)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    label_ids = torch.argmax(outputs.logits, dim = 1)\n",
    "    print(label_ids)\n",
    "    \n",
    "    labels = [model.config.id2label[label_id] for label_id in label_ids.tolist()]\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a1a94-6ede-407c-bf84-844ee6bd84dc",
   "metadata": {},
   "source": [
    "#### Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa3878e6-6d28-4c8d-be93-a79894657686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ee90e9-fde4-4d24-86a8-fe3b7d19f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Prepare dataset\n",
    "#2. Load pretrained Tokenizer, call it with dataset -> encoding\n",
    "#3. Build Pytorch Dataset with encodings\n",
    "#4. Load pretrained Model\n",
    "#5. a) Load Trainer and train it\n",
    "#   b) or use native Pytorch training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "664fd767-5d10-4722-92fe-28d7b468b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "def read_imdb_split(split_dir):\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in [\"pos\",\"neg\"]:\n",
    "        for text_file in (split_dir,label_dir).iterdir():\n",
    "            texts.append(text_file.read_text())\n",
    "            labels.append(0 if label_dir == \"neg\" else 1)\n",
    "            \n",
    "    return texts, labels\n",
    "\n",
    "#large movie review datasets\n",
    "#http://ai.standford.edu/~amaas/data/sentiment\n",
    "\n",
    "# train_texts, val_texts, train_lables , val_lables = train_test_split(train_texts, train_labels, test_size = 0.2)\n",
    "\n",
    "# #class IMbDastsset():\n",
    "# #.\n",
    "# #........\n",
    "\n",
    "\n",
    "\n",
    "# tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# train_encodings = tokenizer(train_texts, truncation = True, padding = True)\n",
    "# val_encodings = tokenizer(val_texts, truncation = True, padding = True)\n",
    "# test_encodings = tokenizer(test_texts, truncation = True, padding = True)\n",
    "\n",
    "\n",
    "# train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "# val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "# test_dataset = IMDbDataset(test_encodings, test_labels)\n",
    "\n",
    "# training_args = TrainingArguments (\n",
    "    \n",
    "\n",
    "    \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61850f41-a466-4d69-96fc-b35aaa0cb1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# from transformers import AdamW\n",
    "\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "# model.to(device)\n",
    "# model.train()\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size = 16, shuffle = True)\n",
    "\n",
    "# optim = AdamW(model.parameters(), lr = 5e-5)\n",
    "\n",
    "# num.........\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# outputs = model(input_ids, attention_mask = attention_mask, labels = labels)\n",
    "\n",
    "#https://www.youtube.com/watch?v=GSt00_-0ncQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d97b6-0cac-4c18-8794-ed962d3cef9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
