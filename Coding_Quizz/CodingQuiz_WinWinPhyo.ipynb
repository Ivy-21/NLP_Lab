{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6sxIWW3siDIX",
   "metadata": {
    "id": "6sxIWW3siDIX"
   },
   "source": [
    "## Coding Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rMXUaBoFiDIa",
   "metadata": {
    "id": "rMXUaBoFiDIa"
   },
   "source": [
    "With the given dataset, Please compare your best possible version of\n",
    "\n",
    "    (1) BiLSTM,\n",
    "    (2) BiLSTM with multiplicative attention (you have to fix e), and\n",
    "    (3) BERT\n",
    "\n",
    "Report the accuracy, precision, recall, and f1-score of each model.\n",
    "\n",
    "For (1) and (2), use the following hyperparameters:\n",
    "\n",
    "    Optimizer: SG\n",
    "    Embedding: GloVe (https://pytorch.org/text/stable/vocab.html#torchtext.vocab.GloVe) >> Please change the embed_dim accordingly.\n",
    "    Epochs: 2\n",
    "    Batch size: 32\n",
    "    Save the model with the best params\n",
    "\n",
    "Anything not stated, please assume accordingly\n",
    "\n",
    "\n",
    "For (2), Multiplicative attention differs from the General Attention (in Assignment 4) such that, for the *Alignment Scores* (or Energy), we multiply the Keys with some weights first before we dot the Keys with the Query.\n",
    "\n",
    "$\\mathbf{e}_i = \\mathbf{q}^T \\ \\mathbf{W}  \\mathbf{k}_t $\n",
    "\n",
    "where $ \\mathbf{W} \\in \\mathbb{R}^{h,h}$\n",
    "\n",
    "* Hint : The shape of the Keys before and after multiplying with the weights should be the same\n",
    "\n",
    "For (3), use this tutorial https://huggingface.co/docs/transformers/training as your guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dmecyLQ_iDIb",
   "metadata": {
    "id": "dmecyLQ_iDIb"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "Kv2R99AJiDIc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kv2R99AJiDIc",
    "outputId": "47e20ef1-66de-45b4-f94a-56613f40ee2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9-X63XRXiDId",
   "metadata": {
    "id": "9-X63XRXiDId"
   },
   "source": [
    "#### 1. Load the IMDB Review dataset from TorchText (https://pytorch.org/text/stable/datasets.html#id10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "G41iEK2V7yvk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G41iEK2V7yvk",
    "outputId": "31ef0712-e337-4984-a7e8-1491b99cce0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'are', 'learning', 'torchtext', 'in', 'U.K.', '!']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "tokens = tokenizer(\"We are learning torchtext in U.K.!\")  #some test\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "NvMMrR0P76yN",
   "metadata": {
    "id": "NvMMrR0P76yN"
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "# vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "Q91K9-A4iDId",
   "metadata": {
    "id": "Q91K9-A4iDId"
   },
   "outputs": [],
   "source": [
    "# text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "# label_pipeline = lambda x: 1 if x == 'pos' else 0\n",
    "\n",
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence #++\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        length_list.append(processed_text.size(0))  #++<-----packed padded sequences require length\n",
    "    #criterion expects float labels\n",
    "    return torch.tensor(label_list, dtype=torch.float64), pad_sequence(text_list, padding_value=pad_idx, batch_first=True), torch.tensor(length_list, dtype=torch.int64)\n",
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "train_iter = IMDB(split='train')\n",
    "test_iter = IMDB(split='test')\n",
    "\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.15)\n",
    "num_val = int(len(train_dataset) * 0.10)\n",
    "num_test = int(len(test_dataset) * 0.05)\n",
    "\n",
    "split_train_, split_valid_, _ = \\\n",
    "    random_split(train_dataset, [num_train, num_val,len(train_dataset)- num_train - num_val])\n",
    "\n",
    "split_test_, _ = \\\n",
    "    random_split(train_dataset, [num_test, len(test_dataset) - num_test])\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(split_train_, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(split_valid_, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_loader = DataLoader(split_test_, batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: 1 if x == 'pos' else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "JAj73Y5H860G",
   "metadata": {
    "id": "JAj73Y5H860G"
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText\n",
    "fast_vectors = FastText('simple')\n",
    "\n",
    "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab.get_itos()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "u2LvEj0o8Tah",
   "metadata": {
    "id": "u2LvEj0o8Tah"
   },
   "outputs": [],
   "source": [
    "input_dim = len(vocab)\n",
    "hidden_dim = 256\n",
    "embed_dim = 300\n",
    "output_dim = 1\n",
    "\n",
    "pad_idx = vocab['<pad>']\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "\n",
    "\n",
    "num_epochs = 2\n",
    "lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "MimF7zW58VwV",
   "metadata": {
    "id": "MimF7zW58VwV"
   },
   "outputs": [],
   "source": [
    "#explicitly initialize weights for better learning\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.RNN):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param) #<---here\n",
    "                \n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "kXHWzWO28YgR",
   "metadata": {
    "id": "kXHWzWO28YgR"
   },
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train() #useful for batchnorm and dropout\n",
    "    for i, (label, text, text_length) in enumerate(loader): \n",
    "        label = label.to(device) #(batch_size, )\n",
    "        text = text.to(device) #(batch_size, seq len)\n",
    "                \n",
    "        #predict\n",
    "        predictions = model(text, text_length) #output by the fc is (batch_size, 1), thus need to remove this 1\n",
    "        predictions = predictions.squeeze(1)\n",
    "        \n",
    "        #calculate loss\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = binary_accuracy(predictions, label)\n",
    "        \n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "        if i == 10:\n",
    "            break\n",
    "                \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (label, text, text_length) in enumerate(loader): \n",
    "            label = label.to(device) #(batch_size, )\n",
    "            text = text.to(device) #(batch_size, seq len)\n",
    "\n",
    "            predictions = model(text, text_length)\n",
    "            predictions = predictions.squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, label)\n",
    "            acc = binary_accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "            if i == 10:\n",
    "                break\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "yyZ8i-lv8a1R",
   "metadata": {
    "id": "yyZ8i-lv8a1R"
   },
   "outputs": [],
   "source": [
    "class new_LSTM_cell(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, lstm_type: str):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm_type = lstm_type\n",
    "        \n",
    "        # initialise the trainable Parameters\n",
    "        self.U_i = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_i = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        self.U_f = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_f = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        self.U_g = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_g = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_g = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        self.U_o = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_o = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        if self.lstm_type == 'peephole' :\n",
    "            self.P_i = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "            self.P_f = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "            self.P_o = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "            \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, x, init_states=None):\n",
    "        bs, seq_len, _ = x.shape\n",
    "        output = []\n",
    "        \n",
    "        # initialize the hidden state and cell state for the first time step \n",
    "        if init_states is None:\n",
    "            h_t  = torch.zeros(bs, self.hidden_dim).to(x.device)\n",
    "            c_t  = torch.zeros(bs, self.hidden_dim).to(x.device)\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "        \n",
    "        # For each time step of the input x, do ...\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :] # get x data of time step t (SHAPE: (batch_size, input_dim))\n",
    "            \n",
    "            if self.lstm_type in ['vanilla', 'coupled'] :\n",
    "                f_t = torch.sigmoid(    h_t @ self.W_f  +  x_t @ self.U_f  +  self.b_f)\n",
    "                o_t = torch.sigmoid(    h_t @ self.W_o  +  x_t @ self.U_o  +  self.b_o)\n",
    "                if self.lstm_type == 'vanilla':\n",
    "                    i_t = torch.sigmoid(    h_t @ self.W_i  +  x_t @ self.U_i  +  self.b_i)\n",
    "                if self.lstm_type == 'coupled':\n",
    "                    i_t = (1 - f_t)\n",
    "            if self.lstm_type == 'peephole' :\n",
    "                i_t = torch.sigmoid( h_t @ self.W_i + x_t @ self.U_i + c_t @ self.P_i + self.b_i) # SHAPE: (batch_size, hidden_dim)\n",
    "                f_t = torch.sigmoid( h_t @ self.W_f + x_t @ self.U_f + c_t @ self.P_f + self.b_f) # SHAPE: (batch_size, hidden_dim)\n",
    "                o_t = torch.sigmoid( h_t @ self.W_o + x_t @ self.U_o + c_t @ self.P_o + self.b_o) # SHAPE: (batch_size, hidden_dim)\n",
    "            \n",
    "            g_t = torch.tanh(       h_t @ self.W_g  +  x_t @ self.U_g   + self.b_g)\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            \n",
    "            output.append(h_t.unsqueeze(0)) # reshape h_t to (1, batch_size, hidden_dim), then append to the list of hidden states\n",
    "\n",
    "        output = torch.cat(output, dim = 0) # concatenate h_t of all time steps into SHAPE :(seq_len, batch_size, hidden_dim)\n",
    "        output = output.transpose(0, 1).contiguous() # just transpose to SHAPE :(seq_len, batch_size, hidden_dim)\n",
    "        return output, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f8H7FoHK8dVu",
   "metadata": {
    "id": "f8H7FoHK8dVu"
   },
   "outputs": [],
   "source": [
    "class BiLSTM_model(nn.Module):\n",
    "    def __init__(self, input_dim: int, embed_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.num_directions = 2\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=pad_idx)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.forward_lstm   =  new_LSTM_cell(embed_dim, hidden_dim, lstm_type = 'vanilla')\n",
    "        self.backward_lstm  =  new_LSTM_cell(embed_dim, hidden_dim, lstm_type = 'vanilla')\n",
    "        \n",
    "        # These should be torch Parameters\n",
    "        self.W_h = nn.Parameter(torch.Tensor(hidden_dim*self.num_directions, hidden_dim*self.num_directions ))\n",
    "        self.b_h = nn.Parameter(torch.Tensor(hidden_dim*self.num_directions))\n",
    "        \n",
    "        self.fc  = nn.Linear(hidden_dim*self.num_directions, output_dim)\n",
    "    \n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded      = self.embedding(text)\n",
    "        embedded_flip =  torch.flip(embedded, [1]) \n",
    "        \n",
    "        output_forward, (hn_forward, cn_forward)    = self.forward_lstm(embedded, init_states=None)\n",
    "        output_backward, (hn_backward, cn_backward) = self.backward_lstm(embedded_flip, init_states=None)\n",
    "        \n",
    "        concat_hn = torch.cat( (hn_forward, hn_backward), dim=1 ) \n",
    "        ht        = torch.sigmoid( concat_hn @ self.W_h + self.b_h)\n",
    "\n",
    "        return self.fc(ht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "NHmMSO3u8gVZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NHmMSO3u8gVZ",
    "outputId": "625bb34d-6cf1-4b42-c4bb-5479b4fbc0de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 0.082 | Train Acc: 4.63%\n",
      "\t Val. Loss: 0.128 |  Val. Acc: 6.49%\n",
      "Epoch: 02 | Train Loss: 0.084 | Train Acc: 4.42%\n",
      "\t Val. Loss: 0.125 |  Val. Acc: 6.57%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "bilstm = BiLSTM_model(input_dim, embed_dim, hidden_dim, output_dim).to(device)\n",
    "bilstm.apply(initialize_weights)\n",
    "bilstm.embedding.weight.data = fast_embedding\n",
    "\n",
    "optimizer = optim.SGD(bilstm.parameters(), lr=lr) \n",
    "criterion = nn.BCEWithLogitsLoss() #combine sigmoid with binary cross entropy\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(bilstm, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(bilstm, valid_loader, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_accs.append(valid_acc)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "# del bilstm\n",
    "# del optimizer\n",
    "# del criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "vA9yla8kD-6B",
   "metadata": {
    "id": "vA9yla8kD-6B"
   },
   "outputs": [],
   "source": [
    "def metrics(model, ds, thresh):\n",
    "  # accuracy  = (TP + TN)  / N\n",
    "  # precision = TP / (TP + FP)\n",
    "  # recall    = TP / (TP + FN)\n",
    "  # F1        = 2 / [(1 / precision) + (1 / recall)]\n",
    "\n",
    "    tp = 0; tn = 0; fp = 0; fn = 0\n",
    "    for i in range(len(ds)):\n",
    "    inpts = ds[i]['predictors']  # dictionary style\n",
    "    target = ds[i]['sex']    # float32  [0.0] or [1.0]\n",
    "    # with T.no_grad():\n",
    "    #   p = model(inpts)       # between 0.0 and 1.0\n",
    "\n",
    "    # should really avoid 'target == 1.0'\n",
    "    if target == 1.0 and p > thresh:    # TP\n",
    "        tp += 1\n",
    "    elif target == 1.0 and p < thresh:   # FP\n",
    "        fp += 1\n",
    "    elif target == 0.0 and p > thresh:   # TN\n",
    "        tn += 1\n",
    "    elif target == 0.0 and p < thresh:  # FN\n",
    "        fn += 1\n",
    "\n",
    "    N = tp + fp + tn + fn\n",
    "    if N != len(ds):\n",
    "    print(\"FATAL LOGIC ERROR\")\n",
    "\n",
    "    accuracy = (tp + tn) / (N * 1.0)\n",
    "    precision = (1.0 * tp) / (tp + fp)\n",
    "    recall = (1.0 * tp) / (tp + fn)\n",
    "    f1 = 2.0 / ((1.0 / precision) + (1.0 / recall))\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "UmHt3ZI2EUOO",
   "metadata": {
    "id": "UmHt3ZI2EUOO"
   },
   "outputs": [],
   "source": [
    "metrics_LSTM = (bilstm, test_loader, 0.5)\n",
    "# print(type(metrics_LSTM))\n",
    "# print(metrics_LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mAebHt6AED7-",
   "metadata": {
    "id": "mAebHt6AED7-"
   },
   "source": [
    "### LSTM Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "JKkDNRer8ii_",
   "metadata": {
    "id": "JKkDNRer8ii_"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LSTM_GAtt(nn.Module):\n",
    "    def __init__(self, input_dim: int, embed_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        # let's use pytorch's LSTM\n",
    "        self.lstm = nn.LSTM(embed_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=num_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        # Linear Layer for binary classification \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.W = nn.Linear(hidden_dim, hidden_dim)\n",
    "    def attention_net(self, lstm_output, hn):\n",
    "        \n",
    "        h_t      = hn.unsqueeze(2)\n",
    "        H_keys   = torch.clone(lstm_output)\n",
    "        H_values = torch.clone(lstm_output)\n",
    "        H_query = torch.clone(lstm_output)\n",
    "        \n",
    "        \n",
    "        alignment_score   = torch.bmm(H_keys, h_t).squeeze(2) # SHAPE : (bs, seq_len, 1)\n",
    "        # score = torch.bmm(self.W, H_keys)\n",
    "        # # score  = self.W @  H_keys\n",
    "        # #alignment_score = (score @  H_query.T).squeeze(2)\n",
    "        # alignment_score = (torch.bmm(self.W, H_keys).squeeze(2)\n",
    "        \n",
    "        soft_attn_weights = F.softmax(alignment_score, 1) # SHAPE : (bs, seq_len, 1)\n",
    "        \n",
    "        context           = torch.bmm(H_values.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2) # SHAPE : (bs, hidden_size * num_directions)\n",
    "        \n",
    "        return context\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "\n",
    "        embedded = self.embedding(text) # SHAPE : (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        lstm_output, (hn, cn) = self.lstm(embedded)\n",
    "        \n",
    "        # This is how we concatenate the forward hidden and backward hidden from Pytorch's BiLSTM\n",
    "        hn = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "\n",
    "        attn_output = self.attention_net(lstm_output, hn)\n",
    "        \n",
    "        return self.fc(attn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6i0-Uk2aALPn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6i0-Uk2aALPn",
    "outputId": "8dd3c5ce-a8a0-4738-8bb7-d38f5b5f4768"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 0.065 | Train Acc: 4.66%\n",
      "\t Val. Loss: 0.097 |  Val. Acc: 6.65%\n",
      "Epoch: 02 | Train Loss: 0.065 | Train Acc: 4.32%\n",
      "\t Val. Loss: 0.096 |  Val. Acc: 7.75%\n"
     ]
    }
   ],
   "source": [
    "g_attmodel = LSTM_GAtt(input_dim, embed_dim, hidden_dim, output_dim).to(device)\n",
    "g_attmodel.apply(initialize_weights)\n",
    "g_attmodel.embedding.weight.data = fast_embedding\n",
    "\n",
    "optimizer = optim.Adam(g_attmodel.parameters(), lr=lr) #<----changed to Adam\n",
    "criterion = nn.BCEWithLogitsLoss() #combine sigmoid with binary cross entropy\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train(g_attmodel, train_loader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(g_attmodel, valid_loader, criterion)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_accs.append(valid_acc)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "# del g_attmodel\n",
    "# del optimizer\n",
    "# del criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "-K4WzVH-FbJi",
   "metadata": {
    "id": "-K4WzVH-FbJi"
   },
   "outputs": [],
   "source": [
    "def metrics(model, ds, thresh):\n",
    "  # accuracy  = (TP + TN)  / N\n",
    "  # precision = TP / (TP + FP)\n",
    "  # recall    = TP / (TP + FN)\n",
    "  # F1        = 2 / [(1 / precision) + (1 / recall)]\n",
    "\n",
    "    tp = 0; tn = 0; fp = 0; fn = 0\n",
    "    for i in range(len(ds)):\n",
    "    inpts = ds[i]['predictors']  # dictionary style\n",
    "    target = ds[i]['sex']    # float32  [0.0] or [1.0]\n",
    "    # with T.no_grad():\n",
    "    #   p = model(inpts)       # between 0.0 and 1.0\n",
    "\n",
    "    # should really avoid 'target == 1.0'\n",
    "    if target == 1.0 and p > thresh:    # TP\n",
    "        tp += 1\n",
    "    elif target == 1.0 and p < thresh:   # FP\n",
    "        fp += 1\n",
    "    elif target == 0.0 and p > thresh:   # TN\n",
    "        tn += 1\n",
    "    elif target == 0.0 and p < thresh:  # FN\n",
    "        fn += 1\n",
    "\n",
    "    N = tp + fp + tn + fn\n",
    "    if N != len(ds):\n",
    "    print(\"FATAL LOGIC ERROR\")\n",
    "\n",
    "    accuracy = (tp + tn) / (N * 1.0)\n",
    "    precision = (1.0 * tp) / (tp + fp)\n",
    "    recall = (1.0 * tp) / (tp + fn)\n",
    "    f1 = 2.0 / ((1.0 / precision) + (1.0 / recall))\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ctfXdGhsFcIg",
   "metadata": {
    "id": "ctfXdGhsFcIg"
   },
   "outputs": [],
   "source": [
    "metrics_LSTM2 = (g_attmodel, test_loader, 0.5)\n",
    "# print(type(metrics_LSTM))\n",
    "#print(metrics_LSTM2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qqTbv_e3EHgG",
   "metadata": {
    "id": "qqTbv_e3EHgG"
   },
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bmpKBzGwACzp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bmpKBzGwACzp",
    "outputId": "3eb965e8-c817-424d-ed96-a971dd844154"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cEsQ7iY4_tEz",
   "metadata": {
    "id": "cEsQ7iY4_tEz"
   },
   "outputs": [],
   "source": [
    "### BERT\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "# tokenized_train_datasets = train_dataset.map(tokenize_function)\n",
    "# tokenized_test_datasets = test_dataset.map(tokenize_function, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "lJpUGG0lAsT3",
   "metadata": {
    "id": "lJpUGG0lAsT3"
   },
   "outputs": [],
   "source": [
    "#model_BERT = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "q9RqPTUUCOor",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9RqPTUUCOor",
    "outputId": "16d8c29e-dc1f-4162-bc8a-e39af90fdd20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#from transformers import AutoTokenizer, AutoModelForSequenceClassification, DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "\n",
    "model_name =  \"bert-base-cased\"\n",
    "model_BERT = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "#tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "qeNJHs_yGVeE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qeNJHs_yGVeE",
    "outputId": "9f9d7d3e-b2fc-4931-978e-1a497c68f14d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "414 Client Error: Request-URI Too Large for url: https://huggingface.co/BertForSequenceClassification(%0A%20%20(bert):%20BertModel(%0A%20%20%20%20(embeddings):%20BertEmbeddings(%0A%20%20%20%20%20%20(word_embeddings):%20Embedding(28996,%20768,%20padding_idx=0)%0A%20%20%20%20%20%20(position_embeddings):%20Embedding(512,%20768)%0A%20%20%20%20%20%20(token_type_embeddings):%20Embedding(2,%20768)%0A%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20)%0A%20%20%20%20(encoder):%20BertEncoder(%0A%20%20%20%20%20%20(layer):%20ModuleList(%0A%20%20%20%20%20%20%20%20(0):%20BertLayer(%0A%20%20%20%20%20%20%20%20%20%20(attention):%20BertAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20(self):%20BertSelfAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(query):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(key):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(value):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20(output):%20BertSelfOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(intermediate):%20BertIntermediate(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=3072,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(output):%20BertOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=3072,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20(1):%20BertLayer(%0A%20%20%20%20%20%20%20%20%20%20(attention):%20BertAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20(self):%20BertSelfAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(query):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(key):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(value):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20(output):%20BertSelfOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(intermediate):%20BertIntermediate(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=3072,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(output):%20BertOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=3072,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20(2):%20BertLayer(%0A%20%20%20%20%20%20%20%20%20%20(attention):%20BertAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20(self):%20BertSelfAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(query):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(key):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(value):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20(output):%20BertSelfOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(intermediate):%20BertIntermediate(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=3072,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(output):%20BertOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=3072,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20(3):%20BertLayer(%0A%20%20%20%20%20%20%20%20%20%20(attention):%20BertAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20(self):%20BertSelfAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(query):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(key):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(value):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20(output):%20BertSelfOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(intermediate):%20BertIntermediate(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=3072,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(output):%20BertOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=3072,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20(4):%20BertLayer(%0A%20%20%20%20%20%20%20%20%20%20(attention):%20BertAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20(self):%20BertSelfAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(query):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(key):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(value):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20(output):%20BertSelfOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(intermediate):%20BertIntermediate(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=3072,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(output):%20BertOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=3072,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20(5):%20BertLayer(%0A%20%20%20%20%20%20%20%20%20%20(attention):%20BertAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20(self):%20BertSelfAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(query):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(key):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(value):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20(output):%20BertSelfOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(intermediate):%20BertIntermediate(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=3072,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(output):%20BertOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=3072,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20(6):%20BertLayer(%0A%20%20%20%20%20%20%20%20%20%20(attention):%20BertAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20(self):%20BertSelfAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(query):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(key):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(value):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20(output):%20BertSelfOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(intermediate):%20BertIntermediate(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=3072,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(output):%20BertOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=3072,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20(7):%20BertLayer(%0A%20%20%20%20%20%20%20%20%20%20(attention):%20BertAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20(self):%20BertSelfAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(query):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(key):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(value):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20(output):%20BertSelfOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(intermediate):%20BertIntermediate(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=3072,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(output):%20BertOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=3072,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20(8):%20BertLayer(%0A%20%20%20%20%20%20%20%20%20%20(attention):%20BertAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20(self):%20BertSelfAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(query):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(key):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(value):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20(output):%20BertSelfOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(intermediate):%20BertIntermediate(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=3072,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(output):%20BertOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=3072,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20(9):%20BertLayer(%0A%20%20%20%20%20%20%20%20%20%20(attention):%20BertAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20(self):%20BertSelfAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(query):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(key):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(value):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20(output):%20BertSelfOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(intermediate):%20BertIntermediate(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=3072,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(output):%20BertOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=3072,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20(10):%20BertLayer(%0A%20%20%20%20%20%20%20%20%20%20(attention):%20BertAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20(self):%20BertSelfAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(query):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(key):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(value):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20(output):%20BertSelfOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(intermediate):%20BertIntermediate(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=3072,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(output):%20BertOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=3072,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20(11):%20BertLayer(%0A%20%20%20%20%20%20%20%20%20%20(attention):%20BertAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20(self):%20BertSelfAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(query):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(key):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(value):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20(output):%20BertSelfOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(intermediate):%20BertIntermediate(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=3072,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20(output):%20BertOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=3072,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20)%0A%20%20%20%20)%0A%20%20%20%20(pooler):%20BertPooler(%0A%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20(activation):%20Tanh()%0A%20%20%20%20)%0A%20%20)%0A%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20(classifier):%20Linear(in_features=768,%20out_features=2,%20bias=True)%0A)/resolve/main/config.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1853\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1854\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1855\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   2049\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2050\u001b[0;31m             \u001b[0m_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2051\u001b[0m             \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X-Linked-Etag\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36m_raise_for_status\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m   1976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1977\u001b[0;31m     \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 414 Client Error: Request-URI Too Large for url: https://huggingface.co/BertForSequenceClassification(%0A%20%20(bert):%20BertModel(%0A%20%20%20%20(embeddings):%20BertEmbeddings(%0A%20%20%20%20%20%20(word_embeddings):%20Embedding(28996,%20768,%20padding_idx=0)%0A%20%20%20%20%20%20(position_embeddings):%20Embedding(512,%20768)%0A%20%20%20%20%20%20(token_type_embeddings):%20Embedding(2,%20768)%0A%20%20%20%20%20%20(LayerNorm):%20LayerNorm((768,),%20eps=1e-12,%20elementwise_affine=True)%0A%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20)%0A%20%20%20%20(encoder):%20BertEncoder(%0A%20%20%20%20%20%20(layer):%20ModuleList(%0A%20%20%20%20%20%20%20%20(0):%20BertLayer(%0A%20%20%20%20%20%20%20%20%20%20(attention):%20BertAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20(self):%20BertSelfAttention(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(query):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(key):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(value):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dropout):%20Dropout(p=0.1,%20inplace=False)%0A%20%20%20%20%20%20%20%20%20%20%20%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20(output):%20BertSelfOutput(%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20(dense):%20Linear(in_features=768,%20out_features=768,%20bias=True)%0A%20%20%2...",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-e9c5de88fb28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_BERT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 config = AutoConfig.from_pretrained(\n\u001b[0;32m--> 471\u001b[0;31m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m                 )\n\u001b[1;32m    473\u001b[0m             \u001b[0mconfig_tokenizer_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name_or_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mtrust_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trust_remote_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;31m# That config file may point us toward another config file to use.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m             raise EnvironmentError(\n\u001b[0;32m--> 619\u001b[0;31m                 \u001b[0;34m\"We couldn't connect to 'https://huggingface.co/' to load this model and it looks like \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m                 \u001b[0;34mf\"{pretrained_model_name_or_path} is not the path to a directory conaining a {configuration_file} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m                 \u001b[0;34m\"file.\\nCheckout your internet connection or see how to run the library in offline mode at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co/' to load this model and it looks like BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n) is not the path to a directory conaining a config.json file.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_BERT)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CodingQuiz.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
