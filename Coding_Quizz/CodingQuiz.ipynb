{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sxIWW3siDIX"
      },
      "source": [
        "## Coding Quiz"
      ],
      "id": "6sxIWW3siDIX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMXUaBoFiDIa"
      },
      "source": [
        "With the given dataset, Please compare your best possible version of\n",
        "\n",
        "    (1) BiLSTM,\n",
        "    (2) BiLSTM with multiplicative attention (you have to fix e), and\n",
        "    (3) BERT\n",
        "\n",
        "Report the accuracy, precision, recall, and f1-score of each model.\n",
        "\n",
        "For (1) and (2), use the following hyperparameters:\n",
        "\n",
        "    Optimizer: SG\n",
        "    Embedding: GloVe (https://pytorch.org/text/stable/vocab.html#torchtext.vocab.GloVe) >> Please change the embed_dim accordingly.\n",
        "    Epochs: 2\n",
        "    Batch size: 32\n",
        "    Save the model with the best params\n",
        "\n",
        "Anything not stated, please assume accordingly\n",
        "\n",
        "\n",
        "For (2), Multiplicative attention differs from the General Attention (in Assignment 4) such that, for the *Alignment Scores* (or Energy), we multiply the Keys with some weights first before we dot the Keys with the Query.\n",
        "\n",
        "$\\mathbf{e}_i = \\mathbf{q}^T \\ \\mathbf{W}  \\mathbf{k}_t $\n",
        "\n",
        "where $ \\mathbf{W} \\in \\mathbb{R}^{h,h}$\n",
        "\n",
        "* Hint : The shape of the Keys before and after multiplying with the weights should be the same\n",
        "\n",
        "For (3), use this tutorial https://huggingface.co/docs/transformers/training as your guide."
      ],
      "id": "rMXUaBoFiDIa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmecyLQ_iDIb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
        "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
      ],
      "id": "dmecyLQ_iDIb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kv2R99AJiDIc",
        "outputId": "512bc56d-8edd-479d-a03f-2158bb1ff571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torchtext\n",
        "import torch\n",
        "from torch import nn\n",
        "import math\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "id": "Kv2R99AJiDIc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-X63XRXiDId"
      },
      "source": [
        "#### 1. Load the IMDB Review dataset from TorchText (https://pytorch.org/text/stable/datasets.html#id10)"
      ],
      "id": "9-X63XRXiDId"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q91K9-A4iDId"
      },
      "outputs": [],
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: 1 if x == 'pos' else 0\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence #++\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, length_list = [], [], []\n",
        "    for (_label, _text) in batch:\n",
        "        label_list.append(label_pipeline(_label))\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        length_list.append(processed_text.size(0))  #++<-----packed padded sequences require length\n",
        "    #criterion expects float labels\n",
        "    return torch.tensor(label_list, dtype=torch.float64), pad_sequence(text_list, padding_value=pad_idx, batch_first=True), torch.tensor(length_list, dtype=torch.int64)\n",
        "\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "train_iter = IMDB(split='train')\n",
        "test_iter = IMDB(split='test')\n",
        "\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "num_train = int(len(train_dataset) * 0.15)\n",
        "num_val = int(len(train_dataset) * 0.10)\n",
        "num_test = int(len(test_dataset) * 0.05)\n",
        "\n",
        "split_train_, split_valid_, _ = \\\n",
        "    random_split(train_dataset, [num_train, num_val,len(train_dataset)- num_train - num_val])\n",
        "\n",
        "split_test_, _ = \\\n",
        "    random_split(train_dataset, [num_test, len(test_dataset) - num_test])\n",
        "\n",
        "train_loader = DataLoader(split_train_, batch_size=batch_size,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_loader = DataLoader(split_valid_, batch_size=batch_size,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(split_test_, batch_size=batch_size,\n",
        "                             shuffle=True, collate_fn=collate_batch)"
      ],
      "id": "Q91K9-A4iDId"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "thesis",
      "language": "python",
      "name": "thesis"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "CodingQuiz.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}